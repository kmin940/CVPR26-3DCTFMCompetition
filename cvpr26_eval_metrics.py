"""
Evaluate metrics from per-sample predictions CSV file.
This script reads the per-sample predictions CSV generated by inference.py
and computes the same metrics to verify reproducibility.
"""

import argparse
import os
import pandas as pd
import torch
import numpy as np
from torchmetrics import (
    Accuracy,
    F1Score,
    AUROC,
    AveragePrecision,
    Recall,
    Specificity,
    MetricCollection,
)
from metrics.balanced_accuracy import BalancedAccuracy


def build_metrics(num_classes, device):
    """Build metric collection matching inference.py"""
    # Alghough our CVPR26 challenge ranking only considers balanced_acc, auroc, we provide the full set of metrics.
    base_metrics = MetricCollection({
        "acc": Accuracy(task="multiclass", num_classes=num_classes),
        "f1": F1Score(task="multiclass", num_classes=num_classes, average="macro"),
        "auroc": AUROC(task="multiclass", num_classes=num_classes),
        "ap": AveragePrecision(task="multiclass", num_classes=num_classes),
        "sensitivity": Recall(task="multiclass", num_classes=num_classes, average="macro"),
        "specificity": Specificity(task="multiclass", num_classes=num_classes, average="macro"),
        "balanced_acc": BalancedAccuracy(num_classes=num_classes, task="multiclass"),
    }).to(device)
    return base_metrics


def main():
    ap = argparse.ArgumentParser(
        description="Evaluate metrics from per-sample predictions CSV"
    )
    ap.add_argument(
        "--csv_path",
        type=str,
        default="/home/jma/Documents/cryoSumin/CT_FM/data/embeddings/features_ct_fm_merlin/adrenal_hyperplasia/results3/test_per_sample_predictions.csv",
        help="Path to per-sample predictions CSV file (e.g., test_per_sample_predictions.csv)",
    )
    args = ap.parse_args()

    # Read per-sample predictions
    print(f"Loading predictions from: {args.csv_path}")
    df = pd.read_csv(args.csv_path)

    # Extract labels and predictions
    labels = torch.tensor(df['label'].values).long()
    predictions = torch.tensor(df['prediction'].values).long()

    # Extract logits (columns starting with 'logit_class_')
    logit_cols = [col for col in df.columns if col.startswith('logit_class_')]
    logit_cols = sorted(logit_cols, key=lambda x: int(x.split('_')[-1]))
    logits = torch.tensor(df[logit_cols].values).float()

    # Extract probabilities (columns starting with 'prob_class_')
    prob_cols = [col for col in df.columns if col.startswith('prob_class_')]
    prob_cols = sorted(prob_cols, key=lambda x: int(x.split('_')[-1]))
    probs = torch.tensor(df[prob_cols].values).float()

    num_classes = len(logit_cols)
    print(f"Number of samples: {len(df)}")
    print(f"Number of classes: {num_classes}")

    # Setup device and metrics
    device = "cuda" if torch.cuda.is_available() else "cpu"
    metrics = build_metrics(num_classes, device)

    # Move data to device
    logits = logits.to(device)
    labels = labels.to(device)

    # Compute metrics
    print("\nComputing metrics...")
    metrics.update(logits, labels)
    computed = metrics.compute()

    # Convert to CPU dict
    computed_cpu = {}
    for k, v in computed.items():
        computed_cpu[k] = v.item() if torch.is_tensor(v) else v

    # Print metrics
    print("\nRecomputed metrics from CSV:")
    for k, v in computed_cpu.items():
        print(f"  {k}: {v:.4f}")


    df_metrics = pd.DataFrame([computed_cpu]).round(4)
    output_csv = os.path.join(
        args.csv_path.replace('_per_sample_predictions.csv', '_metrics_recomputed.csv')
    )
    df_metrics.to_csv(output_csv, index=False)
    print(f'\nSaved recomputed metrics to {output_csv}')

    # Compare with original metrics if available
    original_metrics_path = args.csv_path.replace('_per_sample_predictions.csv', '_metrics.csv')
    if os.path.exists(original_metrics_path):
        print(f"\nComparing with original metrics from {original_metrics_path}")
        df_original = pd.read_csv(original_metrics_path)

        print("\nComparison:")
        print("-" * 60)
        print(f"{'Metric':<20} {'Original':<15} {'Recomputed':<15} {'Match'}")
        print("-" * 60)

        all_match = True
        for metric in computed_cpu.keys():
            if metric in df_original.columns:
                original_val = df_original[metric].values[0]
                recomputed_val = computed_cpu[metric]
                match = np.isclose(original_val, recomputed_val, rtol=1e-4, atol=1e-6)
                all_match = all_match and match
                match_str = "✓" if match else "✗"
                print(f"{metric:<20} {original_val:<15.4f} {recomputed_val:<15.4f} {match_str}")

        print("-" * 60)
        if all_match:
            print("✓ All metrics match!")
        else:
            print("✗ Some metrics do not match. This may be due to numerical precision.")


if __name__ == "__main__":
    main()
